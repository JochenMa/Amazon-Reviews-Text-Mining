{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oUm5eUCs1OD5",
    "outputId": "38016067-9b0a-46de-8eb7-5582724ec627"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bK1MhGbs1-Bp",
    "outputId": "c159b1a5-ed85-4385-f869-e26c7d8c2417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting html.parser\n",
      "  Downloading html-parser-0.2.tar.gz (904 bytes)\n",
      "Collecting ply\n",
      "  Downloading ply-3.11-py2.py3-none-any.whl (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 2.8 MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: html.parser\n",
      "  Building wheel for html.parser (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for html.parser: filename=html_parser-0.2-py3-none-any.whl size=1328 sha256=8695fba4c0589e456a72ce9b59fb3576192fb6511809c6aab73e871f4f1f763f\n",
      "  Stored in directory: /root/.cache/pip/wheels/11/86/38/0554afea46105c70bae8d223c427bba371aa0c83ce88d57b27\n",
      "Successfully built html.parser\n",
      "Installing collected packages: ply, html.parser\n",
      "Successfully installed html.parser-0.2 ply-3.11\n",
      "Collecting pattern3\n",
      "  Downloading pattern3-3.0.0.tar.gz (23.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 23.7 MB 63.6 MB/s \n",
      "\u001b[?25hRequirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from pattern3) (4.6.3)\n",
      "Collecting cherrypy\n",
      "  Downloading CherryPy-18.6.1-py2.py3-none-any.whl (419 kB)\n",
      "\u001b[K     |████████████████████████████████| 419 kB 38.2 MB/s \n",
      "\u001b[?25hCollecting docx\n",
      "  Downloading docx-0.2.4.tar.gz (54 kB)\n",
      "\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n",
      "\u001b[?25hCollecting feedparser\n",
      "  Downloading feedparser-6.0.8-py3-none-any.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 8.1 MB/s \n",
      "\u001b[?25hCollecting pdfminer3k\n",
      "  Downloading pdfminer3k-1.3.4-py3-none-any.whl (100 kB)\n",
      "\u001b[K     |████████████████████████████████| 100 kB 10.3 MB/s \n",
      "\u001b[?25hCollecting simplejson\n",
      "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 48.3 MB/s \n",
      "\u001b[?25hCollecting pdfminer.six\n",
      "  Downloading pdfminer.six-20211012-py3-none-any.whl (5.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 5.6 MB 32.1 MB/s \n",
      "\u001b[?25hCollecting cheroot>=8.2.1\n",
      "  Downloading cheroot-8.5.2-py2.py3-none-any.whl (97 kB)\n",
      "\u001b[K     |████████████████████████████████| 97 kB 6.3 MB/s \n",
      "\u001b[?25hCollecting portend>=2.1.1\n",
      "  Downloading portend-3.1.0-py3-none-any.whl (5.3 kB)\n",
      "Collecting jaraco.collections\n",
      "  Downloading jaraco.collections-3.4.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.7/dist-packages (from cherrypy->pattern3) (8.12.0)\n",
      "Collecting zc.lockfile\n",
      "  Downloading zc.lockfile-2.0-py2.py3-none-any.whl (9.7 kB)\n",
      "Collecting jaraco.functools\n",
      "  Downloading jaraco.functools-3.4.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from cheroot>=8.2.1->cherrypy->pattern3) (1.15.0)\n",
      "Collecting tempora>=1.8\n",
      "  Downloading tempora-4.1.2-py3-none-any.whl (15 kB)\n",
      "Requirement already satisfied: pytz in /usr/local/lib/python3.7/dist-packages (from tempora>=1.8->portend>=2.1.1->cherrypy->pattern3) (2018.9)\n",
      "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from docx->pattern3) (4.2.6)\n",
      "Requirement already satisfied: Pillow>=2.0 in /usr/local/lib/python3.7/dist-packages (from docx->pattern3) (7.1.2)\n",
      "Collecting sgmllib3k\n",
      "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
      "Collecting jaraco.classes\n",
      "  Downloading jaraco.classes-3.2.1-py3-none-any.whl (5.6 kB)\n",
      "Collecting jaraco.text\n",
      "  Downloading jaraco.text-3.6.0-py3-none-any.whl (8.1 kB)\n",
      "Requirement already satisfied: importlib-resources in /usr/local/lib/python3.7/dist-packages (from jaraco.text->jaraco.collections->cherrypy->pattern3) (5.4.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.7/dist-packages (from importlib-resources->jaraco.text->jaraco.collections->cherrypy->pattern3) (3.6.0)\n",
      "Requirement already satisfied: chardet in /usr/local/lib/python3.7/dist-packages (from pdfminer.six->pattern3) (3.0.4)\n",
      "Collecting cryptography\n",
      "  Downloading cryptography-36.0.1-cp36-abi3-manylinux_2_24_x86_64.whl (3.6 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.6 MB 23.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography->pdfminer.six->pattern3) (1.15.0)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography->pdfminer.six->pattern3) (2.21)\n",
      "Requirement already satisfied: ply in /usr/local/lib/python3.7/dist-packages (from pdfminer3k->pattern3) (3.11)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zc.lockfile->cherrypy->pattern3) (57.4.0)\n",
      "Building wheels for collected packages: pattern3, docx, sgmllib3k\n",
      "  Building wheel for pattern3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pattern3: filename=pattern3-3.0.0-py2.py3-none-any.whl size=18554351 sha256=6391ee477e4e0b149c0e08d1f1093e02399156b208875e0b92d98ec27cd59b89\n",
      "  Stored in directory: /root/.cache/pip/wheels/96/6b/e9/a93fc12f8f71cdf14f42e45b54ef9d036fd1f00c0b749762ff\n",
      "  Building wheel for docx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for docx: filename=docx-0.2.4-py3-none-any.whl size=53924 sha256=b45d33444c7a7cb33701302acbc26d88a71bc925c630aa35d0664c946a8abfd0\n",
      "  Stored in directory: /root/.cache/pip/wheels/dc/ac/45/846e21706415c92fc6e0964e343be1eb31cfc63e149dee5fbe\n",
      "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6065 sha256=0187ce7a09b21a2b4a533672e6287128ef7c37f606724aeacaf7314861943868\n",
      "  Stored in directory: /root/.cache/pip/wheels/73/ad/a4/0dff4a6ef231fc0dfa12ffbac2a36cebfdddfe059f50e019aa\n",
      "Successfully built pattern3 docx sgmllib3k\n",
      "Installing collected packages: jaraco.functools, tempora, jaraco.text, jaraco.classes, zc.lockfile, sgmllib3k, portend, jaraco.collections, cryptography, cheroot, simplejson, pdfminer3k, pdfminer.six, feedparser, docx, cherrypy, pattern3\n",
      "Successfully installed cheroot-8.5.2 cherrypy-18.6.1 cryptography-36.0.1 docx-0.2.4 feedparser-6.0.8 jaraco.classes-3.2.1 jaraco.collections-3.4.0 jaraco.functools-3.4.0 jaraco.text-3.6.0 pattern3-3.0.0 pdfminer.six-20211012 pdfminer3k-1.3.4 portend-3.1.0 sgmllib3k-1.0.0 simplejson-3.17.6 tempora-4.1.2 zc.lockfile-2.0\n",
      "Collecting pyLDAvis\n",
      "  Downloading pyLDAvis-3.3.1.tar.gz (1.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
      "Collecting numpy>=1.20.0\n",
      "  Downloading numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
      "\u001b[K     |████████████████████████████████| 15.7 MB 14.5 MB/s \n",
      "\u001b[?25hRequirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
      "Collecting funcy\n",
      "  Downloading funcy-1.16-py2.py3-none-any.whl (32 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
      "Collecting pandas>=1.2.0\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 11.3 MB 18.2 MB/s \n",
      "\u001b[?25hRequirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.0.0)\n",
      "Building wheels for collected packages: pyLDAvis\n",
      "  Building wheel for pyLDAvis (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyLDAvis: filename=pyLDAvis-3.3.1-py2.py3-none-any.whl size=136897 sha256=385167b314aa6e3278abf38593aac4a0b7e19a7d67d01d060844ebb1e290fc78\n",
      "  Stored in directory: /root/.cache/pip/wheels/c9/21/f6/17bcf2667e8a68532ba2fbf6d5c72fdf4c7f7d9abfa4852d2f\n",
      "Successfully built pyLDAvis\n",
      "Installing collected packages: numpy, pandas, funcy, pyLDAvis\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.19.5\n",
      "    Uninstalling numpy-1.19.5:\n",
      "      Successfully uninstalled numpy-1.19.5\n",
      "  Attempting uninstall: pandas\n",
      "    Found existing installation: pandas 1.1.5\n",
      "    Uninstalling pandas-1.1.5:\n",
      "      Successfully uninstalled pandas-1.1.5\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
      "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.5 which is incompatible.\n",
      "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
      "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
      "Successfully installed funcy-1.16 numpy-1.21.4 pandas-1.3.5 pyLDAvis-3.3.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/past/types/oldstr.py:5: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Iterable\n",
      "/usr/local/lib/python3.7/dist-packages/past/builtins/misc.py:4: DeprecationWarning: Using or importing the ABCs from 'collections' instead of from 'collections.abc' is deprecated since Python 3.3,and in 3.9 it will stop working\n",
      "  from collections import Mapping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  ['<', 'p', '>', 'The', 'circus', 'dog', 'in', 'a', 'plissé', 'skirt', 'jumped', 'over', 'Python', 'who', 'was', \"n't\", 'that', 'large', ',', 'just', '3', 'feet', 'long.', '<', '/p', '>']\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plissé skirt jumped over Python who was not that large, just 3 feet long.</p>\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /root/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  [('<', 'a'), ('p', 'n'), ('>', 'v'), ('the', None), ('circus', 'n'), ('dog', 'n'), ('in', None), ('a', None), ('plissé', 'n'), ('skirt', 'n'), ('jumped', 'v'), ('over', None), ('python', 'n'), ('who', None), ('was', 'v'), (\"n't\", 'r'), ('that', None), ('large', 'a'), (',', None), ('just', 'r'), ('3', None), ('feet', 'n'), ('long.', 'a'), ('<', 'n'), ('/p', 'n'), ('>', 'n')]\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > the circus dog in a plissé skirt jump over python who be n't that large , just 3 foot long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:    p   The circus dog in a plissé skirt jumped over Python who was n t that large   just 3 feet long     p  \n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  < p > The circus dog plissé skirt jumped Python n't large , 3 feet long. < /p >\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  p The circus dog in a plissé skirt jumped over Python who was n't that large just feet long. /p\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.\n",
      "Original:   <p>The circus dog in a plissé skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n",
      "Processed:  <p>The circus dog in a plisse skirt jumped over Python who wasn't that large, just 3 feet long.</p>\n"
     ]
    }
   ],
   "source": [
    "#packages needed\n",
    "#ignore warnings about future changes in functions as they take too much space\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "#the module 'sys' allows istalling module from inside Jupyter\n",
    "import sys\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "#Natrual Language ToolKit (NLTK)\n",
    "import nltk\n",
    "from sklearn import metrics\n",
    "#text normalization function\n",
    "%run \"/content/drive/MyDrive/Text_Normalization_Function.ipynb\"\n",
    "#ignore warnings about future changes in functions as they take too much space\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4UUuWook2ieY"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import gzip\n",
    "\n",
    "def parse(path):\n",
    "  g = gzip.open(path, 'rb')\n",
    "  for l in g:\n",
    "    yield eval(l)\n",
    "\n",
    "def getDF(path):\n",
    "  i = 0\n",
    "  df = {}\n",
    "  for d in parse(path):\n",
    "    df[i] = d\n",
    "    i += 1\n",
    "  return pd.DataFrame.from_dict(df, orient='index')\n",
    "\n",
    "df = getDF('/content/drive/MyDrive/reviews_Baby_5.json.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "fA1sXdwl2mcy"
   },
   "outputs": [],
   "source": [
    "df.new = df.loc[:,[\"reviewText\",\"overall\"]]\n",
    "\n",
    "a = np.array(df.new.overall)\n",
    "class_list = np.repeat(\"           \",len(a))\n",
    "for i in range(len(a)):\n",
    "    if a[i] == 5.0 or a[i] == 4.0:\n",
    "        class_list[i]=\"High\"\n",
    "    if a[i] == 3.0:\n",
    "        class_list[i] = \"Medium\"\n",
    "    if a[i] == 2.0 or a[i] == 1.0:\n",
    "        class_list[i] = \"Low\"\n",
    "df.new[\"class\"] = class_list\n",
    "\n",
    "df.High = df.new.loc[df.new[\"class\"]==\"High\"]\n",
    "df.Medium = df.new.loc[df.new[\"class\"]==\"Medium\"]\n",
    "df.Low = df.new.loc[df.new[\"class\"]==\"Low\"]\n",
    "\n",
    "import random\n",
    "random.seed(1)\n",
    "High_train_index = random.sample(list(df.High.index),3350)\n",
    "Medium_train_index = random.sample(list(df.Medium.index),3300)\n",
    "Low_train_index = random.sample(list(df.Low.index),3350)\n",
    "\n",
    "High_left = list(df.High.index)\n",
    "for i in High_train_index:\n",
    "    High_left.remove(i)\n",
    "\n",
    "Medium_left = list(df.Medium.index)\n",
    "for i in Medium_train_index:\n",
    "    Medium_left.remove(i)\n",
    "\n",
    "Low_left = list(df.Low.index)\n",
    "for i in Low_train_index:\n",
    "    Low_left.remove(i)\n",
    "    \n",
    "## Get the random index for the test data index; Total test data has 1,000 reviews with each 250\n",
    "High_test_index = random.sample(High_left,335)\n",
    "Medium_test_index = random.sample(Medium_left,353)\n",
    "Low_test_index = random.sample(Low_left,335)\n",
    "\n",
    "\n",
    "df_train = df.new.loc[High_train_index].append(df.new.loc[Medium_train_index],ignore_index=True).append(df.new.loc[Low_train_index],ignore_index= True)\n",
    "\n",
    "df_test = df.new.loc[High_test_index].append(df.new.loc[Medium_test_index],ignore_index = True).append(df.new.loc[Low_test_index], ignore_index = True)\n",
    "\n",
    "text_train = np.array(df_train[\"reviewText\"])\n",
    "class_train = np.array(df_train[\"class\"])\n",
    "text_test = np.array(df_test[\"reviewText\"])\n",
    "class_test = np.array(df_test[\"class\"])\n",
    "\n",
    "x_train_norm = normalize_corpus(text_train)\n",
    "x_test_norm = normalize_corpus(text_test)\n",
    "y_train = class_train\n",
    "y_test = class_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IYSc09Bj2tu8"
   },
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "t9OfSdOc2s3_"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "BOW_matrix = bow_vectorizer.fit_transform(x_train_norm).toarray()\n",
    "#pd.DataFrame(np.round(BOW_matrix,2))\n",
    "\n",
    "x_train_bow = bow_vectorizer.fit_transform(x_train_norm)\n",
    "corpus_train_bow_table = pd.DataFrame(data = x_train_bow.todense(), columns = bow_vectorizer.get_feature_names())\n",
    "#corpus_train_bow_table.head(5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BDbxV78r25yi",
    "outputId": "3fabacf5-ba90-4078-cdb4-2351993f65c0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['3ml', 'absolutely', 'absorption', 'added', 'adhesive',\n",
       "       'allowable', 'already', 'although', 'amazon', 'ameda', 'angle',\n",
       "       'annoy', 'annoying', 'answer', 'anything', 'apart', 'apy',\n",
       "       'armrest', 'asap', 'aspirator', 'attachment', 'attempt', 'awesome',\n",
       "       'awful', 'awkward', 'bad', 'badly', 'bag', 'barely', 'basically',\n",
       "       'basket', 'bassinet', 'battery', 'bebepod', 'beep', 'belt', 'best',\n",
       "       'big', 'bird', 'bit', 'blanket', 'blinking', 'bonus', 'book',\n",
       "       'booster', 'bottom', 'bra', 'bracket', 'break', 'britax', 'broken',\n",
       "       'brush', 'buckle', 'button', 'buy', 'cable', 'call', 'called',\n",
       "       'camera', 'canopy', 'cap', 'car', 'carrier', 'carry', 'cause',\n",
       "       'chair', 'change', 'charge', 'charm', 'cheap', 'cheaply', 'cheapy',\n",
       "       'child', 'classic', 'clay', 'clearly', 'color', 'company',\n",
       "       'compartment', 'complaint', 'completely', 'con', 'concept', 'cons',\n",
       "       'constantly', 'contact', 'cook', 'cooking', 'cosi', 'could',\n",
       "       'cover', 'crappy', 'crook', 'cruz', 'cuddle', 'cup', 'customer',\n",
       "       'cut', 'dangerous', 'daughter', 'dead', 'decent', 'deduct',\n",
       "       'defeat', 'defective', 'definitely', 'description', 'design',\n",
       "       'difficult', 'dining', 'disappoint', 'disappointed',\n",
       "       'disappointing', 'disappointment', 'disconnect', 'divider',\n",
       "       'doesnt', 'dollar', 'double', 'doubler', 'dough', 'downside',\n",
       "       'drawback', 'drawer', 'dry', 'easy', 'either', 'elastic', 'else',\n",
       "       'elsewhere', 'email', 'end', 'enjoy', 'ergobaby', 'even',\n",
       "       'excellent', 'excite', 'expect', 'extension', 'extra', 'facing',\n",
       "       'fail', 'fairly', 'fall', 'fan', 'fantastic', 'far', 'faulty',\n",
       "       'favorite', 'feed', 'fell', 'figure', 'finally', 'fine', 'first',\n",
       "       'fit', 'fix', 'flange', 'flat', 'flaw', 'flexible', 'flimsy',\n",
       "       'fluke', 'fold', 'forever', 'freshener', 'friend', 'front',\n",
       "       'function', 'g2', 'gap', 'garbage', 'gift', 'girl', 'give', 'glad',\n",
       "       'good', 'granddaughter', 'grandson', 'great', 'grow', 'guardian',\n",
       "       'guess', 'gym', 'half', 'hamper', 'handed', 'handle', 'handset',\n",
       "       'handy', 'happen', 'happy', 'hard', 'harness', 'hassle', 'hat',\n",
       "       'hate', 'hazard', 'head', 'headrest', 'hear', 'heavy', 'height',\n",
       "       'help', 'highly', 'holder', 'hole', 'hop', 'hope', 'horrible',\n",
       "       'however', 'humidifier', 'hygeia', 'idea', 'ignite', 'immediately',\n",
       "       'impossible', 'imse', 'inconvenient', 'indie', 'inflate',\n",
       "       'information', 'install', 'instruction', 'interest', 'interested',\n",
       "       'issue', 'item', 'job', 'jumperoo', 'junk', 'karoo', 'keep', 'kg',\n",
       "       'kiddopotamus', 'kind', 'know', 'lack', 'large', 'latch', 'leak',\n",
       "       'leash', 'least', 'leave', 'left', 'lid', 'lifesaver', 'light',\n",
       "       'lightweight', 'like', 'lime', 'liquid', 'lite', 'little', 'lock',\n",
       "       'long', 'look', 'lot', 'loud', 'love', 'lu', 'luck', 'magnetic',\n",
       "       'main', 'material', 'matter', 'maxi', 'may', 'maybe', 'mean',\n",
       "       'mechanism', 'might', 'min', 'minor', 'mirror', 'mist', 'model',\n",
       "       'mold', 'money', 'monitor', 'monitoreverywhere', 'monkey',\n",
       "       'motion', 'motorola', 'move', 'mpis', 'much', 'music', 'nail',\n",
       "       'napper', 'need', 'neither', 'never', 'nice', 'nicely', 'night',\n",
       "       'nipple', 'nothing', 'ok', 'okay', 'open', 'opening', 'option',\n",
       "       'orbit', 'otherwise', 'oven', 'overall', 'overnight', 'packaging',\n",
       "       'page', 'pain', 'paint', 'part', 'pathway', 'pay', 'peel', 'penny',\n",
       "       'perfect', 'perfectly', 'piece', 'pillow', 'pk', 'plastic', 'play',\n",
       "       'pleased', 'plenty', 'pocket', 'point', 'pointless',\n",
       "       'polyurethane', 'poor', 'poorly', 'positive', 'pouch', 'power',\n",
       "       'prefer', 'pretty', 'price', 'pro', 'probably', 'problem',\n",
       "       'product', 'properly', 'pros', 'protector', 'provide', 'purakiki',\n",
       "       'purple', 'purpose', 'push', 'pusher', 'quinny', 'quite', 'radian',\n",
       "       'rather', 'rating', 'read', 'really', 'reason', 'receive',\n",
       "       'recline', 'recliner', 'recommend', 'refund', 'refuse', 'register',\n",
       "       'relatively', 'remote', 'remove', 'replace', 'replacement',\n",
       "       'reply', 'result', 'return', 'review', 'ridiculous', 'rip', 'rnp',\n",
       "       'rocker', 'rough', 'row', 'run', 'sad', 'sadly', 'scary',\n",
       "       'scratchy', 'screen', 'scrub', 'seam', 'seat', 'seatbelt',\n",
       "       'secondly', 'seem', 'select', 'send', 'serve', 'service', 'set',\n",
       "       'shade', 'short', 'si', 'side', 'significant', 'simple', 'size',\n",
       "       'skype', 'sleep', 'slightly', 'slow', 'small', 'smell', 'smile',\n",
       "       'snot', 'soap', 'soft', 'solid', 'something', 'sometimes', 'sony',\n",
       "       'soother', 'sophia', 'sophie', 'space', 'spend', 'spill', 'spoon',\n",
       "       'spout', 'star', 'state', 'static', 'stay', 'stick', 'stiff',\n",
       "       'still', 'stop', 'storage', 'strap', 'straw', 'stroller', 'sturdy',\n",
       "       'submit', 'suck', 'suction', 'suppose', 'sure', 'swing', 'syringe',\n",
       "       'tab', 'take', 'tank', 'tend', 'terrible', 'thermometer', 'thin',\n",
       "       'thing', 'think', 'though', 'throw', 'tight', 'time', 'together',\n",
       "       'total', 'tower', 'toy', 'trash', 'travel', 'tray', 'trip', 'turn',\n",
       "       'unacceptable', 'uncomfortable', 'unfortunately', 'unit', 'unless',\n",
       "       'uppababy', 'upside', 'use', 'useless', 'velcro', 'wagon', 'wally',\n",
       "       'want', 'warning', 'warranty', 'washable', 'waste', 'water', 'way',\n",
       "       'well', 'wheel', 'whole', 'window', 'wish', 'within', 'wonderful',\n",
       "       'work', 'worry', 'worthless', 'wrong', 'yet'], dtype='<U25')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Select best 500 features\n",
    "from sklearn.feature_selection import SelectKBest, chi2, mutual_info_classif\n",
    "\n",
    "chi2_kbest = SelectKBest(score_func = chi2, k =500)\n",
    "x_train_BEST = chi2_kbest.fit_transform(x_train_bow,y_train)\n",
    "\n",
    "chi2_best_features_ind = chi2_kbest.get_support(indices=True)\n",
    "chi2_best_features_names = np.array(bow_vectorizer.get_feature_names())[chi2_best_features_ind]\n",
    "\n",
    "## Best features name\n",
    "chi2_best_features_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "ksX_eQGF27N7"
   },
   "outputs": [],
   "source": [
    "x_train_bow_chi2_BEST_table = pd.DataFrame(data = x_train_BEST.todense(), columns = chi2_best_features_names)\n",
    "#x_train_bow_chi2_BEST_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NnUviia-29sR",
    "outputId": "d3fab46f-643d-4d43-cc41-0d171d650880"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "         High  Medium  Low\n",
      "High     210      38   87\n",
      "Medium    39     233   63\n",
      "Low       57     148  148 \n",
      "\n",
      "Accuracy rate:  0.5777126099706745 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[210,  38,  87],\n",
       "       [ 39, 233,  63],\n",
       "       [ 57, 148, 148]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NORM_x_test_bow = bow_vectorizer.transform(x_test_norm)\n",
    "NORM_x_test_bow_chi2_BEST = chi2_kbest.transform(NORM_x_test_bow)\n",
    "NORM_x_test_bow_chi2_BEST.shape\n",
    "target_names = [\"High\",\"Medium\",'Low']\n",
    "\n",
    "## Naive Bayesian\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "NB_tc = MultinomialNB(alpha=0.1) \n",
    "NB_tc.fit(x_train_BEST, y_train)\n",
    "\n",
    "predicted_nb_chi2_best = NB_tc.predict(NORM_x_test_bow_chi2_BEST)  ## predictions\n",
    "cm_chi2_best = metrics.confusion_matrix(y_test, predicted_nb_chi2_best)\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_chi2_best , \n",
    "                                           columns = target_names,\n",
    "                                           index = target_names),\"\\n\")\n",
    "\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(y_test, predicted_nb_chi2_best),\"\\n\")    \n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_test,predicted_nb_chi2_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eWBmSC_e2_uN",
    "outputId": "0e165f29-3dbd-4424-9d14-2c23c5635ac5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "         High  Medium  Low\n",
      "High     166      68  101\n",
      "Medium    61     173  101\n",
      "Low       76     138  139 \n",
      "\n",
      "Accuracy rate:  0.46725317693059626 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Tree Method\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "Classification_tree = DecisionTreeClassifier()\n",
    "Classification_tree.fit(x_train_BEST, y_train)\n",
    "\n",
    "predicted_tree = Classification_tree.predict(NORM_x_test_bow_chi2_BEST) \n",
    "cm_chi2_best_tree = metrics.confusion_matrix(y_test, predicted_tree)\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_chi2_best_tree , \n",
    "                                           columns = target_names,\n",
    "                                           index = target_names),\"\\n\")\n",
    "\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(y_test, predicted_tree),\"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kaOypp9C3Drm",
    "outputId": "a0dd8e6c-b9a7-4dbf-8583-5031fdc9beea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "         High  Medium  Low\n",
      "High     203      36   96\n",
      "Medium    33     245   57\n",
      "Low       61     148  144 \n",
      "\n",
      "Accuracy rate:  0.5786901270772239 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Random Forest\n",
    "\n",
    "#Import Random Forest Model\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Create a Gaussian Classifier\n",
    "clf=RandomForestClassifier(n_estimators=100)\n",
    "\n",
    "#Train the model using the training sets y_pred=clf.predict(X_test)\n",
    "clf.fit(x_train_BEST,y_train)\n",
    "\n",
    "pred_rf=clf.predict(NORM_x_test_bow_chi2_BEST)\n",
    "\n",
    "cm_chi2_best_randomf = metrics.confusion_matrix(y_test, pred_rf)\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_chi2_best_randomf , \n",
    "                                           columns = target_names,\n",
    "                                           index = target_names),\"\\n\")\n",
    "\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(y_test, pred_rf),\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7j-RGCMe3L-U",
    "outputId": "4e4fb5fa-f725-4fb0-f30c-3ed2f0ad4636"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "         High  Medium  Low\n",
      "High     210      35   90\n",
      "Medium    35     223   77\n",
      "Low       56     145  152 \n",
      "\n",
      "Accuracy rate:  0.5718475073313783 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## SVM \n",
    "\n",
    "from sklearn import linear_model\n",
    "\n",
    "svm = linear_model.SGDClassifier(loss='hinge', random_state = 0) \n",
    "svm.fit(x_train_BEST, y_train)\n",
    "predicted_svm = svm.predict(NORM_x_test_bow_chi2_BEST) \n",
    "cm_chi2_best_svm = metrics.confusion_matrix(y_test, predicted_svm)\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cm_chi2_best_svm , \n",
    "                                           columns = target_names,\n",
    "                                           index = target_names),\"\\n\")\n",
    "\n",
    "print(\"Accuracy rate: \", metrics.accuracy_score(y_test, predicted_svm),\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5V8tPDXy3NyJ",
    "outputId": "8b55ead7-b94b-4f96-d9d1-dbcb60625f7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, None, 32)          320000    \n",
      "                                                                 \n",
      " simple_rnn (SimpleRNN)      (None, None, 32)          2080      \n",
      "                                                                 \n",
      " simple_rnn_1 (SimpleRNN)    (None, None, 32)          2080      \n",
      "                                                                 \n",
      " simple_rnn_2 (SimpleRNN)    (None, None, 32)          2080      \n",
      "                                                                 \n",
      " simple_rnn_3 (SimpleRNN)    (None, 32)                2080      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 328,320\n",
      "Trainable params: 328,320\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "10000 train sequences\n",
      "1023 test sequences\n",
      "Pad sequences (samples x time)\n",
      "X_train shape: (10000, 40)\n",
      "X_test shape: (1023, 40)\n"
     ]
    }
   ],
   "source": [
    "## RNN \n",
    "\n",
    "from keras.layers import SimpleRNN\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, SimpleRNN\n",
    "model = Sequential()\n",
    "model.add(Embedding(10000, 32))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32, return_sequences=True))\n",
    "model.add(SimpleRNN(32))  # This last layer only returns the last outputs.\n",
    "model.summary()\n",
    "\n",
    "X_train=df_train.reviewText.str.strip('=').to_list()\n",
    "Y_train=pd.get_dummies(df_train[\"class\"]).values\n",
    "X_test = df_test.reviewText.str.strip(\"=\").to_list()\n",
    "Y_test = pd.get_dummies(df_test[\"class\"]).values\n",
    "\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# Number of words to consider as features\n",
    "max_features = 1000\n",
    "# Cut texts after this number of words \n",
    "# (among top max_features most common words)\n",
    "maxlen = 40\n",
    "batch_size = 10\n",
    "\n",
    "#Tokenizing\n",
    "tokenizer=Tokenizer(max_features,oov_token=\"<?>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train=tokenizer.texts_to_sequences(X_train)\n",
    "X_test=tokenizer.texts_to_sequences(X_test)\n",
    "print(len(X_train), 'train sequences')\n",
    "print(len(X_test), 'test sequences')\n",
    "\n",
    "# This turns our lists of integers\n",
    "# into a 2D integer tensor of shape `(samples, maxlen)`\n",
    "print('Pad sequences (samples x time)')\n",
    "X_train = pad_sequences(X_train, maxlen=maxlen)\n",
    "X_test = pad_sequences(X_test, maxlen=maxlen)\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9irvKvY43N5f",
    "outputId": "341cdc33-f053-449e-87e3-5d2593558ff7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_2 (Embedding)     (None, None, 32)          32000     \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 40,419\n",
      "Trainable params: 40,419\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, 32))\n",
    "model.add(LSTM(32))\n",
    "model.add(Dense(Y_test.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "#history = model.fit(X_train, Y_train,\n",
    "                    #epochs=20,\n",
    "                    #batch_size=batch_size,\n",
    "                    #validation_data=(X_test,Y_test))\n",
    "model.summary()\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZybxdhGl3QWA",
    "outputId": "1dbfb462-5f82-48e4-b30f-35fa012bbe3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1000/1000 [==============================] - 27s 23ms/step - loss: 0.9660 - acc: 0.5003 - val_loss: 0.9416 - val_acc: 0.5464\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.8461 - acc: 0.6063 - val_loss: 0.8612 - val_acc: 0.6061\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.8048 - acc: 0.6347 - val_loss: 0.8629 - val_acc: 0.6041\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.7813 - acc: 0.6474 - val_loss: 0.8630 - val_acc: 0.6168\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 23s 23ms/step - loss: 0.7619 - acc: 0.6620 - val_loss: 0.8607 - val_acc: 0.6119\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.7387 - acc: 0.6726 - val_loss: 0.8786 - val_acc: 0.6188\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.7171 - acc: 0.6897 - val_loss: 0.8709 - val_acc: 0.6031\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.6913 - acc: 0.7060 - val_loss: 0.9183 - val_acc: 0.5924\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.6653 - acc: 0.7205 - val_loss: 0.9098 - val_acc: 0.5953\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.6398 - acc: 0.7337 - val_loss: 0.9233 - val_acc: 0.5865\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.6135 - acc: 0.7525 - val_loss: 0.9843 - val_acc: 0.5767\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 24s 24ms/step - loss: 0.5848 - acc: 0.7627 - val_loss: 1.0205 - val_acc: 0.5670\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.5552 - acc: 0.7754 - val_loss: 1.0193 - val_acc: 0.5836\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.5246 - acc: 0.7925 - val_loss: 1.0592 - val_acc: 0.5777\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.4951 - acc: 0.8087 - val_loss: 1.1065 - val_acc: 0.5728\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.4639 - acc: 0.8209 - val_loss: 1.1527 - val_acc: 0.5875\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 23s 23ms/step - loss: 0.4322 - acc: 0.8350 - val_loss: 1.1789 - val_acc: 0.5718\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.4066 - acc: 0.8470 - val_loss: 1.2536 - val_acc: 0.5699\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.3784 - acc: 0.8612 - val_loss: 1.3182 - val_acc: 0.5670\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 22s 22ms/step - loss: 0.3478 - acc: 0.8680 - val_loss: 1.3845 - val_acc: 0.5709\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing import sequence\n",
    "from keras import layers\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 32))\n",
    "model.add(layers.Bidirectional(layers.LSTM(32)))\n",
    "model.add(layers.Dense(Y_test.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "history = model.fit(X_train, Y_train, epochs=20, batch_size=10, validation_data = (X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_f5Tf1VlwqmV",
    "outputId": "6c908956-4412-4dbe-b0aa-89d07c8d9f4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 40, 128)           128000    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 34, 32)            28704     \n",
      "                                                                 \n",
      " max_pooling1d (MaxPooling1D  (None, 11, 32)           0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 11, 32)            0         \n",
      "                                                                 \n",
      " conv1d_1 (Conv1D)           (None, 5, 32)             7200      \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 3)                 99        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 164,003\n",
      "Trainable params: 164,003\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/rmsprop.py:130: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(RMSprop, self).__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000/1000 [==============================] - 13s 7ms/step - loss: 1.0952 - acc: 0.3677 - val_loss: 1.0951 - val_acc: 0.3666\n",
      "Epoch 2/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 1.0809 - acc: 0.4270 - val_loss: 1.0804 - val_acc: 0.4223\n",
      "Epoch 3/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 1.0399 - acc: 0.4856 - val_loss: 1.0306 - val_acc: 0.4663\n",
      "Epoch 4/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.9684 - acc: 0.5383 - val_loss: 0.9727 - val_acc: 0.5367\n",
      "Epoch 5/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.9121 - acc: 0.5690 - val_loss: 0.9409 - val_acc: 0.5474\n",
      "Epoch 6/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.8752 - acc: 0.5931 - val_loss: 0.9203 - val_acc: 0.5767\n",
      "Epoch 7/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.8489 - acc: 0.6092 - val_loss: 0.9238 - val_acc: 0.5728\n",
      "Epoch 8/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.8250 - acc: 0.6288 - val_loss: 0.9048 - val_acc: 0.5855\n",
      "Epoch 9/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.8046 - acc: 0.6380 - val_loss: 0.8882 - val_acc: 0.5963\n",
      "Epoch 10/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7882 - acc: 0.6506 - val_loss: 0.8852 - val_acc: 0.5914\n",
      "Epoch 11/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7712 - acc: 0.6596 - val_loss: 0.8849 - val_acc: 0.5914\n",
      "Epoch 12/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7567 - acc: 0.6691 - val_loss: 0.8893 - val_acc: 0.5836\n",
      "Epoch 13/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.7398 - acc: 0.6792 - val_loss: 0.8845 - val_acc: 0.5943\n",
      "Epoch 14/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7247 - acc: 0.6887 - val_loss: 0.8846 - val_acc: 0.5943\n",
      "Epoch 15/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.7109 - acc: 0.6930 - val_loss: 0.8841 - val_acc: 0.6012\n",
      "Epoch 16/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6930 - acc: 0.7063 - val_loss: 0.8919 - val_acc: 0.5943\n",
      "Epoch 17/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6795 - acc: 0.7129 - val_loss: 0.8917 - val_acc: 0.6012\n",
      "Epoch 18/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6593 - acc: 0.7213 - val_loss: 0.8967 - val_acc: 0.6012\n",
      "Epoch 19/20\n",
      "1000/1000 [==============================] - 7s 7ms/step - loss: 0.6451 - acc: 0.7341 - val_loss: 0.8975 - val_acc: 0.6031\n",
      "Epoch 20/20\n",
      "1000/1000 [==============================] - 6s 6ms/step - loss: 0.6249 - acc: 0.7432 - val_loss: 0.9088 - val_acc: 0.5982\n"
     ]
    }
   ],
   "source": [
    "## Convoluational 1D(CNN)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from keras.layers import Dropout\n",
    "\n",
    "model = Sequential()\n",
    "model.add(layers.Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(layers.Conv1D(32, 7,activation='relu'))\n",
    "model.add(layers.MaxPooling1D(3))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(layers.Conv1D(32, 7, activation='relu'))\n",
    "model.add(layers.GlobalMaxPooling1D())\n",
    "model.add(layers.Dense(Y_test.shape[1], activation='softmax'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=1e-4),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['acc'])\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    epochs=20, # Originally 10, changed to 20 \n",
    "                    batch_size=10,\n",
    "                    validation_data = (X_test,Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJvHY7ak3T8t"
   },
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7AacaLhO3Saj",
    "outputId": "86712124-0c80-4eaa-b898-fd6b76ee37cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.4)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.4)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.1)\n",
      "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.4)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
      "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.0.0)\n",
      "Requirement already satisfied: pyLDAvis in /usr/local/lib/python3.7/dist-packages (3.3.1)\n",
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (3.6.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (57.4.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.21.4)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.1.0)\n",
      "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.16.0)\n",
      "Requirement already satisfied: funcy in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (1.3.5)\n",
      "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.0->pyLDAvis) (2018.9)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim->pyLDAvis) (5.2.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->pyLDAvis) (2.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->pyLDAvis) (3.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:File `'./Text_Normalization_Function.ipynb.py'` not found.\n"
     ]
    }
   ],
   "source": [
    "#the module 'sys' allows istalling module from inside Jupyter\n",
    "import sys\n",
    "\n",
    "!{sys.executable} -m pip install numpy\n",
    "import numpy as np\n",
    "\n",
    "!{sys.executable} -m pip install pandas\n",
    "import pandas as pd\n",
    "\n",
    "#Natrual Language ToolKit (NLTK)\n",
    "!{sys.executable} -m pip install nltk\n",
    "import nltk\n",
    "\n",
    "!{sys.executable} -m pip install sklearn\n",
    "from sklearn import metrics\n",
    "#from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import  CountVectorizer #bag-of-words vectorizer \n",
    "from sklearn.decomposition import LatentDirichletAllocation #package for LDA\n",
    "\n",
    "# Plotting tools\n",
    "\n",
    "from pprint import pprint\n",
    "!{sys.executable} -m pip install pyLDAvis #visualizing LDA\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#define text normalization function\n",
    "%run ./Text_Normalization_Function.ipynb #defining text normalization function\n",
    "\n",
    "#ignore warnings about future changes in functions as they take too much space\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "wT99zrBv3Slu"
   },
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        print(\"Topic %d:\" % (topic_idx))\n",
    "        print(\" \".join([feature_names[i]\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n",
    "        \n",
    "def get_topic_words(vectorizer, lda_model, n_words):\n",
    "    keywords = np.array(vectorizer.get_feature_names())\n",
    "    topic_words = []\n",
    "    for topic_weights in lda_model.components_:\n",
    "        top_word_locs = (-topic_weights).argsort()[:n_words]\n",
    "        topic_words.append(keywords.take(top_word_locs).tolist())\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uPdag8g93SrM"
   },
   "outputs": [],
   "source": [
    "#define the bag-of-words vectorizer:\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "#vectorize the normalized data:\n",
    "bow_toy_corpus = bow_vectorizer.fit_transform(x_train_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I5uVpDZZ3oN_",
    "outputId": "0a7d8113-96b9-4f49-a764-0d43995cd481"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.4)\n",
      "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!{sys.executable} -m pip install gensim\n",
    "import gensim\n",
    "\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "pNyJi5Pp3oQ7"
   },
   "outputs": [],
   "source": [
    "bow_vectorizer = CountVectorizer()\n",
    "bow_corpus = bow_vectorizer.fit_transform(x_train_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fhZFCv8t3rmJ"
   },
   "source": [
    "# LDA 2-Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "PCRWbvrT3oWV"
   },
   "outputs": [],
   "source": [
    "lda_corpus_2 = LatentDirichletAllocation(n_components=2, max_iter=500,\n",
    "                                           doc_topic_prior = 0.9,\n",
    "                                           topic_word_prior = 0.9).fit(bow_toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ARYJ0fUE3oY8",
    "outputId": "e7a754b0-5ce4-4041-f97b-f450b0dca7eb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "baby use like love month diaper buy little great really well old good toy time\n",
      "Topic 1:\n",
      "use seat bottle like work baby easy cup stroller time car well great good buy\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 15\n",
    "display_topics(lda_corpus_2, bow_vectorizer.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DNXSWiiw3obx",
    "outputId": "6a1aa40f-e246-44b4-cf41-d405fc840a3a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      000sf      00am   06scion  ...    zurich     zzzzs  생각되어bib을\n",
      "0  0.000004  0.000004  0.000004  ...  0.000008  0.000007  0.000004\n",
      "1  0.000008  0.000008  0.000008  ...  0.000004  0.000004  0.000008\n",
      "\n",
      "[2 rows x 15386 columns]\n"
     ]
    }
   ],
   "source": [
    "word_weights_2 = lda_corpus_2.components_ / lda_corpus_2.components_.sum(axis=1)[:, np.newaxis]\n",
    "print(pd.DataFrame(word_weights_2.T, index = bow_vectorizer.get_feature_names()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Q80KxLlS4SgD"
   },
   "outputs": [],
   "source": [
    "#tokenizing the corpus\n",
    "news_corpus_tokenized = [tokenize_text(x_train_norm[doc_id]) for doc_id in range(len(x_train_norm))]\n",
    "\n",
    "#Dictionary of the corpus:\n",
    "news_dictionary = Dictionary(news_corpus_tokenized)\n",
    "\n",
    "#Bag-of-words representation for each document of the corpus:\n",
    "news_corpus_bow = [news_dictionary.doc2bow(doc) for doc in news_corpus_tokenized]\n",
    "\n",
    "#top 20 words for each topic (using the function defined in session prep)\n",
    "topic_topwords = get_topic_words(vectorizer = bow_vectorizer, lda_model = lda_corpus_2, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3j0LpZF4SjX",
    "outputId": "01b08c87-0018-4d0c-a347-9ff65f574c93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score for the model:  -1.49\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_topwords, \n",
    "                    corpus = news_corpus_bow , \n",
    "                    dictionary = news_dictionary, coherence='u_mass')\n",
    "print(\"Coherence score for the model: \", np.round(cm.get_coherence(), 2))  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "trPzm2bp4SlR",
    "outputId": "bf9c0283-d824-430a-8f3e-da794aad5caf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score by topic (higher values are better):  [-1.49 -1.5 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence score by topic (higher values are better): \", np.round(cm.get_coherence_per_topic(),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iPbM6CgR45dG",
    "outputId": "e909532c-5d25-4f2a-c322-90f3648e8e67"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -3315730.103538731\n"
     ]
    }
   ],
   "source": [
    "print(\"Log-Likelihood (higher values are better): \", lda_corpus_2.score(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vbVXuy7o45fu",
    "outputId": "5bd0db5c-fd64-46e3-d695-b52d0b687236"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (lower values are better):  1599.7215725335789\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (lower values are better): \", lda_corpus_2.perplexity(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OsotNiRk5JJy"
   },
   "source": [
    "# LDA 3-Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "SS27HfFc45h3"
   },
   "outputs": [],
   "source": [
    "lda_corpus_3 = LatentDirichletAllocation(n_components=3, max_iter=500,\n",
    "                                           doc_topic_prior = 0.9,\n",
    "                                           topic_word_prior = 0.9).fit(bow_toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KQovvEdG45kI",
    "outputId": "0e71987b-dde3-4675-e82b-22c8c15d41f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "use baby diaper bag like well buy fit good cover wash great soft crib size\n",
      "Topic 1:\n",
      "seat baby like use love toy old stroller month car easy little son great really\n",
      "Topic 2:\n",
      "use bottle baby work like cup time pump great buy good water well really monitor\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 15\n",
    "display_topics(lda_corpus_3, bow_vectorizer.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TloRT3V65ys3",
    "outputId": "0435fd16-932e-4883-dd35-580abc7a69b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      000sf      00am   06scion  ...    zurich     zzzzs  생각되어bib을\n",
      "0  0.000006  0.000006  0.000007  ...  0.000013  0.000007  0.000012\n",
      "1  0.000005  0.000005  0.000009  ...  0.000005  0.000009  0.000005\n",
      "2  0.000012  0.000012  0.000006  ...  0.000006  0.000006  0.000007\n",
      "\n",
      "[3 rows x 15386 columns]\n"
     ]
    }
   ],
   "source": [
    "word_weights_3 = lda_corpus_3.components_ / lda_corpus_3.components_.sum(axis=1)[:, np.newaxis]\n",
    "print(pd.DataFrame(word_weights_3.T, index = bow_vectorizer.get_feature_names()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "RiAy0He05y0I"
   },
   "outputs": [],
   "source": [
    "#tokenizing the corpus\n",
    "news_corpus_tokenized = [tokenize_text(x_train_norm[doc_id]) for doc_id in range(len(x_train_norm))]\n",
    "\n",
    "#Dictionary of the corpus:\n",
    "news_dictionary = Dictionary(news_corpus_tokenized)\n",
    "\n",
    "#Bag-of-words representation for each document of the corpus:\n",
    "news_corpus_bow = [news_dictionary.doc2bow(doc) for doc in news_corpus_tokenized]\n",
    "\n",
    "#top 20 words for each topic (using the function defined in session prep)\n",
    "topic_topwords = get_topic_words(vectorizer = bow_vectorizer, lda_model = lda_corpus_3, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DAT2L-_I5y2v",
    "outputId": "c5663640-0ed9-4c50-87f2-fc9f8b79d64f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score for the model:  -1.68\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_topwords, \n",
    "                    corpus = news_corpus_bow , \n",
    "                    dictionary = news_dictionary, coherence='u_mass')\n",
    "print(\"Coherence score for the model: \", np.round(cm.get_coherence(), 3))  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CoP0MaRj5y5n",
    "outputId": "ef30c5c4-754f-4128-fd5e-b07d27dd7bc6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score by topic (higher values are better):  [-1.76  -1.436 -1.845]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence score by topic (higher values are better): \", np.round(cm.get_coherence_per_topic(),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3tym11V_57kS",
    "outputId": "91726ccd-5ea3-4700-fa89-457581e2348b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -3321837.523717019\n"
     ]
    }
   ],
   "source": [
    "print(\"Log-Likelihood (higher values are better): \", lda_corpus_3.score(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DzVnka5f57mq",
    "outputId": "edb50890-08f4-45bd-b8c9-901e668b2af6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (lower values are better):  1621.608834455664\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (lower values are better): \", lda_corpus_3.perplexity(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8vjumAF5_JR"
   },
   "source": [
    "# LDA 4-Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "27otYubn57qS"
   },
   "outputs": [],
   "source": [
    "lda_corpus_4 = LatentDirichletAllocation(n_components=4, max_iter=500,\n",
    "                                           doc_topic_prior = 0.9,\n",
    "                                           topic_word_prior = 0.9).fit(bow_toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TPS2yQkc57tN",
    "outputId": "3574ac6c-d6a5-4dc1-a792-5957798b85f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "baby use work sleep time monitor product night light room sound like swing great pillow\n",
      "Topic 1:\n",
      "seat use stroller car easy like baby put fit strap well chair need old child\n",
      "Topic 2:\n",
      "baby love like toy month use old little son buy well really good great soft\n",
      "Topic 3:\n",
      "use bottle diaper bag cup like work time pump baby clean buy well good water\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 15\n",
    "display_topics(lda_corpus_4, bow_vectorizer.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVC3DR8057wk",
    "outputId": "389dc31f-1d97-44af-9bb9-dc8362fdf41a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      000sf      00am   06scion  ...    zurich     zzzzs  생각되어bib을\n",
      "0  0.000020  0.000019  0.000009  ...  0.000012  0.000018  0.000017\n",
      "1  0.000006  0.000006  0.000013  ...  0.000007  0.000007  0.000007\n",
      "2  0.000007  0.000007  0.000007  ...  0.000009  0.000007  0.000007\n",
      "3  0.000007  0.000007  0.000007  ...  0.000009  0.000007  0.000007\n",
      "\n",
      "[4 rows x 15386 columns]\n"
     ]
    }
   ],
   "source": [
    "word_weights_4 = lda_corpus_4.components_ / lda_corpus_4.components_.sum(axis=1)[:, np.newaxis]\n",
    "print(pd.DataFrame(word_weights_4.T, index = bow_vectorizer.get_feature_names()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "FHMApZOl6ld5"
   },
   "outputs": [],
   "source": [
    "#tokenizing the corpus\n",
    "news_corpus_tokenized = [tokenize_text(x_train_norm[doc_id]) for doc_id in range(len(x_train_norm))]\n",
    "\n",
    "#Dictionary of the corpus:\n",
    "news_dictionary = Dictionary(news_corpus_tokenized)\n",
    "\n",
    "#Bag-of-words representation for each document of the corpus:\n",
    "news_corpus_bow = [news_dictionary.doc2bow(doc) for doc in news_corpus_tokenized]\n",
    "\n",
    "#top 20 words for each topic (using the function defined in session prep)\n",
    "topic_topwords = get_topic_words(vectorizer = bow_vectorizer, lda_model = lda_corpus_4, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rTXzh6O6lg6",
    "outputId": "332ae464-5473-44ab-9414-784f5551f6aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score for the model:  -1.7321\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_topwords, \n",
    "                    corpus = news_corpus_bow , \n",
    "                    dictionary = news_dictionary, coherence='u_mass')\n",
    "print(\"Coherence score for the model: \", np.round(cm.get_coherence(), 4))  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 132
    },
    "id": "LAQ2LLoS6ljM",
    "outputId": "42bf86bc-4a2d-421c-91d8-200edfcb455a"
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-52-d4b666c3eb69>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    Coherence score by topic (higher values are better):  [-1.6708 -1.7848 -1.5817 -1.9948]\u001b[0m\n\u001b[0m                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Coherence score by topic (higher values are better):  [-1.6708 -1.7848 -1.5817 -1.9948]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5S7ShaX56ll0",
    "outputId": "7bcd4946-da86-45ac-a5ee-7c88ba397861"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -3334187.0675827833\n"
     ]
    }
   ],
   "source": [
    "print(\"Log-Likelihood (higher values are better): \", lda_corpus_4.score(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01m087TH6loP",
    "outputId": "b36c09e6-41bb-401c-ebfc-373c4c8d4cf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (lower values are better):  1666.7853267598312\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (lower values are better): \", lda_corpus_4.perplexity(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVjkQZ6X6t0Y"
   },
   "source": [
    "# LDA 5-Topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "AlA2Nsof6z8j"
   },
   "outputs": [],
   "source": [
    "lda_corpus_5 = LatentDirichletAllocation(n_components=5, max_iter=500,\n",
    "                                           doc_topic_prior = 0.9,\n",
    "                                           topic_word_prior = 0.9).fit(bow_toy_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yP6vcPzz6tID",
    "outputId": "2b001776-b66d-4cfb-8d78-14be259c651d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "bottle use baby work pump time monitor nipple milk like product good buy need much\n",
      "Topic 1:\n",
      "baby love month toy like old son play little really great daughter buy color cute\n",
      "Topic 2:\n",
      "use cup clean like easy work little well water food open put plastic hold great\n",
      "Topic 3:\n",
      "use diaper bag baby fit like wash cover size well good buy crib soft great\n",
      "Topic 4:\n",
      "seat stroller car use baby like easy strap chair fit child well put take great\n"
     ]
    }
   ],
   "source": [
    "no_top_words = 15\n",
    "display_topics(lda_corpus_5, bow_vectorizer.get_feature_names(), no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u5ScD-v56tG8",
    "outputId": "355eaa5e-279f-42fd-9783-f0943af0466a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      000sf      00am   06scion  ...    zurich     zzzzs  생각되어bib을\n",
      "0  0.000019  0.000019  0.000009  ...  0.000010  0.000010  0.000012\n",
      "1  0.000010  0.000010  0.000009  ...  0.000010  0.000017  0.000012\n",
      "2  0.000009  0.000009  0.000009  ...  0.000009  0.000009  0.000011\n",
      "3  0.000008  0.000008  0.000008  ...  0.000016  0.000008  0.000010\n",
      "4  0.000008  0.000008  0.000016  ...  0.000008  0.000009  0.000009\n",
      "\n",
      "[5 rows x 15386 columns]\n"
     ]
    }
   ],
   "source": [
    "word_weights_5 = lda_corpus_5.components_ / lda_corpus_5.components_.sum(axis=1)[:, np.newaxis]\n",
    "print(pd.DataFrame(word_weights_5.T, index = bow_vectorizer.get_feature_names()).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "b5FR5E6y66KO"
   },
   "outputs": [],
   "source": [
    "#tokenizing the corpus\n",
    "news_corpus_tokenized = [tokenize_text(x_train_norm[doc_id]) for doc_id in range(len(x_train_norm))]\n",
    "\n",
    "#Dictionary of the corpus:\n",
    "news_dictionary = Dictionary(news_corpus_tokenized)\n",
    "\n",
    "#Bag-of-words representation for each document of the corpus:\n",
    "news_corpus_bow = [news_dictionary.doc2bow(doc) for doc in news_corpus_tokenized]\n",
    "\n",
    "#top 20 words for each topic (using the function defined in session prep)\n",
    "topic_topwords = get_topic_words(vectorizer = bow_vectorizer, lda_model = lda_corpus_5, n_words=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lVuJjGqI66M0",
    "outputId": "25a2542d-703a-4e39-904a-e63cb53b93cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score for the model:  -1.83239\n"
     ]
    }
   ],
   "source": [
    "cm = CoherenceModel(topics=topic_topwords, \n",
    "                    corpus = news_corpus_bow , \n",
    "                    dictionary = news_dictionary, coherence='u_mass')\n",
    "print(\"Coherence score for the model: \", np.round(cm.get_coherence(), 5))  # get coherence value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSeCaf2Z66O8",
    "outputId": "3ee0e6fd-3153-43d6-b86a-4b9910cf4332"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coherence score by topic (higher values are better):  [-1.70601 -1.78695 -1.96551 -2.08428 -1.61919]\n"
     ]
    }
   ],
   "source": [
    "print(\"Coherence score by topic (higher values are better): \", np.round(cm.get_coherence_per_topic(),5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pm5YLwAq66Ri",
    "outputId": "6777f496-9781-4a3f-9f28-524c45a94975"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log-Likelihood (higher values are better):  -3351141.091209992\n"
     ]
    }
   ],
   "source": [
    "print(\"Log-Likelihood (higher values are better): \", lda_corpus_5.score(bow_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJy5qkDn66Tq",
    "outputId": "3ee35cf7-1664-4529-f967-4c2421ae49a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity (lower values are better):  1730.8627030607724\n"
     ]
    }
   ],
   "source": [
    "print(\"Perplexity (lower values are better): \", lda_corpus_5.perplexity(bow_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "thUYoVuhJIIv"
   },
   "source": [
    "### Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Z5ikqNocJJ4h",
    "outputId": "7e45eff2-2f18-4f08-ef82-8d0d7462d8fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vzbpn7NZLuxs",
    "outputId": "4911a296-2e25-47f7-ee0d-0752efeca2db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
      "  warnings.warn(\"The twython library has not been installed. \"\n"
     ]
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "LhzaTrL5MCL5"
   },
   "outputs": [],
   "source": [
    "def analyze_sentiment_vader_lexicon(review, threshold1 = -0.3,threshold2 = 0, verbose = False):\n",
    "    scores = analyzer.polarity_scores(review)\n",
    "    if scores['compound'] <= threshold1:\n",
    "        human_sentiment = 'Low'\n",
    "    elif scores['compound'] <= threshold2:\n",
    "        human_sentiment = 'Medium'\n",
    "    else:\n",
    "        human_sentiment = 'High'\n",
    "    if verbose:                             \n",
    "        print('VADER Polarity (human):', human_sentiment)\n",
    "        print('VADER Score:', round(scores['compound'], 2))\n",
    "    return human_sentiment,scores['compound']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7zpYJWh3LweR",
    "outputId": "efb1f539-6caf-43e2-e9f8-1eb7ea294951"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   VADER Polarity  VADER Score\n",
      "0            High       0.9323\n",
      "1            High       0.8860\n",
      "2             Low       0.3612\n",
      "3            High       0.9543\n",
      "4            High       0.9868\n",
      "5            High       0.8126\n",
      "6            High       0.9584\n",
      "7            High       0.9022\n",
      "8            High       0.9042\n",
      "9            High       0.9042\n",
      "10           High       0.8957\n",
      "11            Low       0.1531\n",
      "12           High       0.8271\n",
      "13           High       0.9764\n",
      "14           High       0.8839\n",
      "15           High       0.9630\n",
      "16            Low       0.3818\n",
      "17           High       0.9595\n",
      "18           High       0.9716\n",
      "19           High       0.8642\n",
      "20            Low       0.4404\n",
      "21           High       0.8316\n",
      "22            Low      -0.2263\n",
      "23         Medium       0.5367\n",
      "24         Medium       0.7269\n",
      "25         Medium       0.6901\n",
      "26           High       0.9341\n",
      "27            Low       0.1010\n",
      "28           High       0.8126\n",
      "29           High       0.8807\n"
     ]
    }
   ],
   "source": [
    "VADER_polarity_test = [analyze_sentiment_vader_lexicon(review, threshold1 = 0.5,threshold2 = 0.8) for review in x_test_norm]\n",
    "VADER_polarity_test_df = pd.DataFrame(VADER_polarity_test, columns = ['VADER Polarity','VADER Score'])\n",
    "print(VADER_polarity_test_df.head(30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "vqiJlZKJLyV5"
   },
   "outputs": [],
   "source": [
    "def format(value):\n",
    "    value = str(value)\n",
    "    return value\n",
    "VADER_polarity_test_df['VADER Polarity'] = VADER_polarity_test_df['VADER Polarity'].apply(format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MmH0xbn5L1zb",
    "outputId": "165c2129-5f0a-41c0-feef-47eb141d4e2f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Rate: 0.464 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "print('Accuracy Rate:', np.round(metrics.accuracy_score(class_test, VADER_polarity_test_df['VADER Polarity']), 3),\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hg0s-WN1L2pV",
    "outputId": "912f3f35-746d-4cf9-943d-acaef2c0aa02"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion matrix: \n",
      "         High  Medium  Low\n",
      "High     236      39   60\n",
      "Medium   114     146   75\n",
      "Low      182      78   93 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "cross_table = metrics.confusion_matrix(class_test, VADER_polarity_test_df['VADER Polarity'])\n",
    "print(\"Confusion matrix: \\n\", pd.DataFrame(data = cross_table, columns = [\"High\", \"Medium\", \"Low\"], index = [\"High\", \"Medium\", \"Low\"]),\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Text Mining Final Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
